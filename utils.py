import os
import tempfile
import shutil
import logging
import requests
from datetime import timedelta
from google.cloud import storage
from google.cloud.video import transcoder_v1
from openai import OpenAI
import subprocess
import time

# åˆå§‹åŒ–å®¢æˆ¶ç«¯
client = OpenAI()
storage_client = storage.Client()
transcoder_client = transcoder_v1.TranscoderServiceClient()

# åˆå§‹åŒ–æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- å¸¸æ•¸è¨­å®š ---
VERSION = "v1.6.14" # ç‰ˆæœ¬è™Ÿæ›´æ–°
BUCKET_NAME = "bubblebucket-a1q5lb"
PROJECT_ID = "bubble-dropzone-2-pgxrk7"
LOCATION = "us-central1"
AUDIO_BATCH_SIZE_MB = 24

def format_srt_time(total_seconds):
    """å°‡ç§’æ•¸ç²¾ç¢ºæ ¼å¼åŒ–ç‚º HH:MM:SS,mmm çš„ SRT æ¨™æº–æ™‚é–“æ ¼å¼"""
    hours, remainder = divmod(total_seconds, 3600)
    minutes, seconds = divmod(remainder, 60)
    milliseconds = int((seconds - int(seconds)) * 1000)
    return f"{int(hours):02d}:{int(minutes):02d}:{int(seconds):02d},{milliseconds:03d}"

def get_audio_duration(file_path):
    """ä½¿ç”¨ ffprobe å–å¾—éŸ³æª”çš„ç²¾ç¢ºæ™‚é•· (ç§’)"""
    try:
        cmd = ["ffprobe", "-v", "error", "-show_entries", "format=duration", "-of", "default=noprint_wrappers=1:nokey=1", file_path]
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        return float(result.stdout.strip())
    except (subprocess.CalledProcessError, ValueError) as e:
        logger.error(f"âŒ ç„¡æ³•å–å¾—éŸ³æª”æ™‚é•· {os.path.basename(file_path)}: {e}")
        return 0.0

def extract_base_path_from_url(video_url):
    if video_url.startswith("https://storage.googleapis.com/"):
        gcs_path = video_url.replace("https://storage.googleapis.com/", "")
        return "/".join(gcs_path.split("/")[:-1])
    raise ValueError(f"URL ä¸æ˜¯æœ‰æ•ˆçš„ GCS HTTP URL: {video_url}")

def convert_http_url_to_gcs_uri(http_url):
    if http_url.startswith("https://storage.googleapis.com/"):
        gcs_path = http_url.replace("https://storage.googleapis.com/", "")
        return f"gs://{gcs_path}"
    raise ValueError(f"URL ä¸æ˜¯æœ‰æ•ˆçš„ GCS HTTP URL: {http_url}")

def create_transcoder_job(input_uri, output_folder_uri, job_id):
    logger.info(f"ğŸ¬ å»ºç«‹ Transcoder ä»»å‹™ï¼š{job_id}")
    audio_stream = transcoder_v1.AudioStream(codec="mp3", bitrate_bps=128000, sample_rate_hertz=44100, channel_count=2)
    mux_stream = transcoder_v1.MuxStream(key="audio_only", container="mp3", elementary_streams=["audio_stream"])
    job = transcoder_v1.Job(
        input_uri=input_uri,
        output_uri=output_folder_uri,
        config=transcoder_v1.JobConfig(elementary_streams=[transcoder_v1.ElementaryStream(key="audio_stream", audio_stream=audio_stream)], mux_streams=[mux_stream])
    )
    parent = f"projects/{PROJECT_ID}/locations/{LOCATION}"
    request = transcoder_v1.CreateJobRequest(parent=parent, job=job)
    return transcoder_client.create_job(request=request)

def wait_for_transcoder_job(job_name, timeout_minutes=30):
    """ç­‰å¾… Transcoder ä»»å‹™å®Œæˆ"""
    logger.info(f"â³ ç­‰å¾… Transcoder ä»»å‹™å®Œæˆï¼š{job_name}")
    start_time = time.time()
    while time.time() - start_time < timeout_minutes * 60:
        job = transcoder_client.get_job(name=job_name)
        
        # ç‹€æ…‹å°æ‡‰ï¼š1=PENDING, 2=RUNNING, 3=SUCCEEDED, 4=FAILED
        state_names = {1: "PENDING", 2: "RUNNING", 3: "SUCCEEDED", 4: "FAILED"}
        state_name = state_names.get(job.state, f"UNKNOWN({job.state})")
        logger.info(f"ğŸ“Š ä»»å‹™ç‹€æ…‹ï¼š{state_name}")

        # --- ä¿®æ­£ï¼šä½¿ç”¨æ•¸å­—ä¾†åˆ¤æ–·ç‹€æ…‹ ---
        if job.state == 3: # SUCCEEDED
            logger.info("âœ… Transcoder ä»»å‹™å®Œæˆ")
            return True
        if job.state == 4: # FAILED
            logger.error(f"âŒ Transcoder ä»»å‹™å¤±æ•—: {job.error}")
            return False
            
        time.sleep(30)
    logger.error("â° Transcoder ä»»å‹™è¶…æ™‚")
    return False

def download_audio_from_gcs(gcs_uri, local_path):
    logger.info(f"ğŸ“¥ å¾ GCS ä¸‹è¼‰éŸ³æª”ï¼š{gcs_uri}")
    bucket_name, blob_name = gcs_uri[5:].split("/", 1)
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(blob_name)
    blob.download_to_filename(local_path)
    logger.info(f"âœ… éŸ³æª”ä¸‹è¼‰å®Œæˆ")

def split_audio_file(audio_path, chunk_size_mb):
    logger.info(f"ğŸ”ª åˆ†å‰²éŸ³æª”ï¼š{audio_path}")
    total_duration = get_audio_duration(audio_path)
    if total_duration == 0.0:
        raise RuntimeError(f"ç„¡æ³•å–å¾—éŸ³æª”æ™‚é•·ï¼š{audio_path}")
    file_size_mb = os.path.getsize(audio_path) / 1024 / 1024
    if file_size_mb <= chunk_size_mb:
        return [audio_path]
    
    chunk_duration = (total_duration * chunk_size_mb) / file_size_mb
    num_chunks = int(total_duration / chunk_duration) + 1
    logger.info(f"ğŸ”ª å°‡åˆ†å‰²ç‚º {num_chunks} æ®µï¼Œæ¯æ®µç´„ {chunk_duration:.2f}s")
    
    chunks = []
    base_path = os.path.splitext(audio_path)[0]
    for i in range(num_chunks):
        start_time = i * chunk_duration
        if start_time >= total_duration:
            break
        chunk_path = f"{base_path}_chunk_{i:03d}.mp3"
        cmd = ["ffmpeg", "-y", "-ss", str(start_time), "-i", audio_path, "-t", str(chunk_duration), "-c", "copy", chunk_path]
        subprocess.run(cmd, check=True, capture_output=True)
        chunks.append(chunk_path)
    return chunks

def upload_to_gcs(file_path, blob_path):
    bucket = storage_client.bucket(BUCKET_NAME)
    blob = bucket.blob(blob_path)
    content_type = "application/x-subrip" if file_path.endswith(".srt") else "audio/mpeg"
    blob.upload_from_filename(file_path, content_type=content_type)
    return blob.public_url

def process_video_task(video_url, user_id, task_id, whisper_language, max_segment_mb, webhook_url, prompt):
    logger.info(f"ğŸ“¥ é–‹å§‹è™•ç†ä»»å‹™ {task_id} (ç‰ˆæœ¬: {VERSION})")
    temp_dir = tempfile.mkdtemp()
    try:
        input_gcs_uri = convert_http_url_to_gcs_uri(video_url)
        base_path = extract_base_path_from_url(video_url)
        
        job_id = f"audio-extract-{user_id}-{task_id}"
        output_gcs_folder = f"gs://{base_path}/transcoder/"
        transcoder_job = create_transcoder_job(input_gcs_uri, output_gcs_folder, job_id)
        
        if not wait_for_transcoder_job(transcoder_job.name):
            raise RuntimeError("Transcoder ä»»å‹™å¤±æ•—æˆ–è¶…æ™‚")
            
        output_gcs_uri = f"gs://{base_path}/transcoder/audio_only.mp3"
        audio_path = os.path.join(temp_dir, "full_audio.mp3")
        download_audio_from_gcs(output_gcs_uri, audio_path)
        
        audio_chunks = split_audio_file(audio_path, max_segment_mb)
        
        final_srt_parts = []
        total_duration_offset = 0.0
        for i, chunk_path in enumerate(audio_chunks):
            logger.info(f"ğŸš€ è™•ç†éŸ³æª”æ‰¹æ¬¡ {i+1}/{len(audio_chunks)}")
            with open(chunk_path, "rb") as f:
                transcript = client.audio.transcriptions.create(model="whisper-1", file=f, response_format="verbose_json", language=whisper_language, prompt=prompt or None)
            
            for segment in transcript.segments:
                start_time = segment['start'] + total_duration_offset
                end_time = segment['end'] + total_duration_offset
                
                start_str = format_srt_time(start_time)
                end_str = format_srt_time(end_time)
                
                text = segment['text'].strip()
                final_srt_parts.append((start_str, end_str, text))
            
            chunk_duration = get_audio_duration(chunk_path)
            total_duration_offset += chunk_duration
            logger.info(f"ğŸ“ æ‰¹æ¬¡ {i+1} å®Œæˆã€‚ç´¯è¨ˆ offset: {total_duration_offset:.2f}s")

        if not final_srt_parts:
            raise Exception("æ²’æœ‰ç”¢ç”Ÿä»»ä½•è½‰éŒ„å…§å®¹")

        srt_path = os.path.join(temp_dir, "final.srt")
        with open(srt_path, "w", encoding="utf-8") as f:
            for i, (start, end, text) in enumerate(final_srt_parts):
                f.write(f"{i + 1}\n")
                f.write(f"{start}  {end}\n")
                f.write(f"{text}\n\n")

        srt_blob_path = f"{base_path}/srt/final.srt"
        srt_url = upload_to_gcs(srt_path, srt_blob_path)
        
        payload = {"ä»»å‹™ç‹€æ…‹": "æˆåŠŸ", "srt_url": srt_url, "task_id": task_id, "user_id": user_id}
        requests.post(webhook_url, json=payload, timeout=10)
        logger.info(f"âœ… ä»»å‹™ {task_id} å®Œæˆ")

    except Exception as e:
        logger.error(f"ğŸ”¥ ä»»å‹™ {task_id} è™•ç†éŒ¯èª¤: {e}", exc_info=True)
        payload = {"ä»»å‹™ç‹€æ…‹": f"å¤±æ•—: {str(e)}", "task_id": task_id, "user_id": user_id}
        requests.post(webhook_url, json=payload, timeout=10)
    finally:
        shutil.rmtree(temp_dir)
