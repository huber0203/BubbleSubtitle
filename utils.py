import os
import tempfile
import shutil
import logging
import requests
from datetime import timedelta
from google.cloud import storage
from google.cloud.video import transcoder_v1
from openai import OpenAI
import subprocess
import io
import time

# åˆå§‹åŒ– OpenAI client
client = OpenAI()

# åˆå§‹åŒ– Google Cloud clients
storage_client = storage.Client()
transcoder_client = transcoder_v1.TranscoderServiceClient()

# åˆå§‹åŒ– logger
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

VERSION = "v1.7.0" # å„ªåŒ–å¾Œçš„ç‰ˆæœ¬
BUCKET_NAME = "bubblebucket-a1q5lb"
CHUNK_FOLDER = "chunks"
SRT_FOLDER = "srt"
TRANSCODER_FOLDER = "transcoder"
AUDIO_BATCH_SIZE_MB = 24

# Google Cloud é…ç½®
PROJECT_ID = "bubble-dropzone-2-pgxrk7"
LOCATION = "us-central1"

### --- å„ªåŒ–é–‹å§‹ --- ###

def main_handler(request_data: dict or list):
    """
    ä¸»è¦è™•ç†å…¥å£ï¼Œæ¥æ”¶åŸå§‹è«‹æ±‚è³‡æ–™ï¼Œé€²è¡Œé©—è­‰å’Œåˆ†æ´¾ã€‚
    é€™å€‹å‡½å¼æ›´æœ‰å½ˆæ€§ï¼Œèƒ½è™•ç†ç‰©ä»¶æˆ–é™£åˆ—å½¢å¼çš„è¼¸å…¥ã€‚
    """
    try:
        # 1. å½ˆæ€§è™•ç†è¼¸å…¥è³‡æ–™ï¼Œç›¸å®¹ n8n çš„ [{...}] å’Œæ¨™æº–çš„ {...} æ ¼å¼
        if isinstance(request_data, list) and request_data:
            data = request_data[0]
            logger.info("åµæ¸¬åˆ°è¼¸å…¥ç‚ºé™£åˆ—æ ¼å¼ï¼Œå·²è‡ªå‹•é¸å–ç¬¬ä¸€å€‹ç‰©ä»¶ã€‚")
        elif isinstance(request_data, dict):
            data = request_data
            logger.info("æ¥æ”¶åˆ°æ¨™æº–ç‰©ä»¶æ ¼å¼è¼¸å…¥ã€‚")
        else:
            raise ValueError("è¼¸å…¥è³‡æ–™æ ¼å¼ä¸æ­£ç¢ºï¼Œå¿…é ˆæ˜¯ç‰©ä»¶æˆ–éç©ºé™£åˆ—ã€‚")
        
        logger.info(f"æ¥æ”¶åˆ°çš„ä»»å‹™è³‡æ–™: {data}")

        # 2. é©—è­‰å¿…è¦åƒæ•¸æ˜¯å¦å­˜åœ¨
        required_keys = [
            "video_url", "task_id", "user_id", "user_email", "user_name",
            "user_headpic", "user_lastname", "whisper_language", "prompt",
            "max_segment_mb", "n8n_webhook"
        ]
        
        # å°‡ n8n_webhook é‡æ–°å‘½åç‚º webhook_url ä»¥ç¬¦åˆå…§éƒ¨å‡½å¼ä½¿ç”¨
        if "n8n_webhook" in data:
            data["webhook_url"] = data["n8n_webhook"]

        for key in required_keys:
            if key not in data and key != "n8n_webhook": # webhook_url æ˜¯å…¶æ›¿ä»£å“
                 raise ValueError(f"ç¼ºå°‘å¿…è¦åƒæ•¸: '{key}'")

        # 3. ä½¿ç”¨é—œéµå­—åƒæ•¸è§£åŒ…ï¼Œæ¸…æ™°åœ°å‘¼å«æ ¸å¿ƒé‚è¼¯
        process_video_task_with_transcoder(**data)

    except (ValueError, KeyError) as e:
        logger.error(f"âŒ ä»»å‹™å‰ç½®æª¢æŸ¥å¤±æ•—: {e}", exc_info=True)
        # å¯ä»¥åœ¨æ­¤è™•å¢åŠ ç™¼é€åˆ° Webhook çš„å¤±æ•—é€šçŸ¥
        # (ä½†é€šå¸¸åœ¨æ­¤éšæ®µå¤±æ•—ï¼Œå¯èƒ½é€£ webhook_url éƒ½æ‹¿ä¸åˆ°)
        return {"status": "error", "message": str(e)}, 400
    except Exception as e:
        logger.error(f"âŒ ç™¼ç”Ÿæœªé æœŸçš„åš´é‡éŒ¯èª¤: {e}", exc_info=True)
        return {"status": "error", "message": "å…§éƒ¨ä¼ºæœå™¨éŒ¯èª¤"}, 500

    return {"status": "ok", "message": "ä»»å‹™å·²æˆåŠŸæ¥æ”¶ä¸¦é–‹å§‹è™•ç†"}, 202

### --- å„ªåŒ–çµæŸ --- ###


def process_video_task_with_transcoder(
    video_url, user_id, task_id, whisper_language, max_segment_mb, 
    webhook_url, prompt, user_email, user_name, user_headpic, user_lastname, **kwargs):
    
    # **kwargs æœƒæ¥æ”¶ä»»ä½•é¡å¤–å‚³å…¥çš„åƒæ•¸ï¼Œé¿å…å‡½å¼å‡ºéŒ¯
    
    logger.info(f"ğŸ“¥ é–‹å§‹ä½¿ç”¨ Transcoder è™•ç†å½±ç‰‡ä»»å‹™ {task_id}")
    logger.info(f"ğŸŒ å½±ç‰‡ä¾†æºï¼š{video_url}")
    logger.info(f"ğŸ‘¤ ä½¿ç”¨è€…ï¼š{user_id} ({user_email})")
    logger.info(f"ğŸŒ èªè¨€ï¼š{whisper_language}")
    logger.info(f"ğŸ”” Webhookï¼š{webhook_url}")
    logger.info(f"ğŸ“ æç¤ºè©ï¼š{prompt}")
    logger.info(f"ğŸ§ª ç¨‹å¼ç‰ˆæœ¬ï¼š{VERSION}")

    temp_dir = tempfile.mkdtemp()
    try:
        # (é€™è£¡ä¹‹å¾Œçš„æ‰€æœ‰ç¨‹å¼ç¢¼éƒ½èˆ‡æ‚¨åŸæœ¬çš„ç›¸åŒï¼Œå› æ­¤çœç•¥ä»¥ä¿æŒç°¡æ½”)
        # ... 
        # 1. ç¢ºèªå½±ç‰‡å¯ä»¥è¨ªå•ä¸¦è½‰æ›ç‚º GCS URI
        logger.info("ğŸ” æª¢æŸ¥å½±ç‰‡ URL...")
        headers = {"User-Agent": "Mozilla/5.0"}
        head_resp = requests.head(video_url, allow_redirects=True, headers=headers)
        total_size = int(head_resp.headers.get("Content-Length", 0))
        total_mb = round(total_size / 1024 / 1024, 2)
        logger.info(f"ğŸ“ å½±ç‰‡å¤§å°ï¼š{total_mb} MB")

        # è½‰æ› HTTP URL ç‚º GCS URI
        input_gcs_uri = convert_http_url_to_gcs_uri(video_url)
        if not input_gcs_uri:
            raise RuntimeError(f"ç„¡æ³•è½‰æ›å½±ç‰‡ URL ç‚º GCS URI: {video_url}")
        
        logger.info(f"ğŸ”„ è½‰æ›å¾Œçš„ GCS URIï¼š{input_gcs_uri}")

        # æå–åŸºç¤è·¯å¾‘ç”¨æ–¼çµ„ç¹”æª”æ¡ˆçµæ§‹
        base_path = extract_base_path_from_url(video_url)
        if not base_path:
            raise RuntimeError(f"ç„¡æ³•æå–åŸºç¤è·¯å¾‘: {video_url}")
        
        logger.info(f"ğŸ“ åŸºç¤è·¯å¾‘ï¼š{base_path}")

        # 2. å»ºç«‹ Transcoder ä»»å‹™
        job_id = f"audio-extract-{user_id}-{task_id}"
        output_gcs_folder = f"gs://{base_path}/{TRANSCODER_FOLDER}/"
        
        transcoder_job = create_transcoder_job(input_gcs_uri, output_gcs_folder, job_id)
        if not transcoder_job:
            raise RuntimeError("å»ºç«‹ Transcoder ä»»å‹™å¤±æ•—")

        # 3. ç­‰å¾… Transcoder å®Œæˆ
        job_name = transcoder_job.name
        if not wait_for_transcoder_job(job_name):
            raise RuntimeError("Transcoder ä»»å‹™å¤±æ•—æˆ–è¶…æ™‚")

        # 4. ä¸‹è¼‰è½‰æ›å¾Œçš„éŸ³æª”
        output_gcs_uri = f"gs://{base_path}/{TRANSCODER_FOLDER}/audio_only.mp3"
        audio_path = os.path.join(temp_dir, "full_audio.mp3")
        if not download_audio_from_gcs(output_gcs_uri, audio_path):
            raise RuntimeError("ä¸‹è¼‰éŸ³æª”å¤±æ•—")

        # 5. åˆ†å‰²éŸ³æª”
        audio_chunks = split_audio_file(audio_path, max_segment_mb)
        if not audio_chunks:
            raise RuntimeError("åˆ†å‰²éŸ³æª”å¤±æ•—")

        # 6. è™•ç†éŸ³æª”æ‰¹æ¬¡
        final_srt_parts = []
        total_duration_offset = 0.0
        
        for batch_idx, chunk_path in enumerate(audio_chunks):
            batch_count = batch_idx + 1
            logger.info(f"ğŸš€ è™•ç†éŸ³æª”æ‰¹æ¬¡ {batch_count}/{len(audio_chunks)}")
            
            chunk_name = f"audio_batch_{batch_count:03d}.mp3"
            chunk_blob_path = f"{base_path}/{CHUNK_FOLDER}/{chunk_name}"
            chunk_blob_path_clean = chunk_blob_path.replace(f"{BUCKET_NAME}/", "")
            upload_url = upload_to_gcs(chunk_path, chunk_blob_path_clean)
            logger.info(f"âœ… éŸ³æª”æ‰¹æ¬¡ä¸Šå‚³ï¼š{upload_url}")
            
            with open(chunk_path, "rb") as f:
                transcript = client.audio.transcriptions.create(
                    model="whisper-1",
                    file=f,
                    response_format="verbose_json",
                    language=whisper_language,
                    prompt=prompt or None,
                )
            
            srt_entries = []
            batch_duration = 0.0
            
            if transcript.segments:
                for segment in transcript.segments:
                    start_time = segment['start'] + total_duration_offset
                    end_time = segment['end'] + total_duration_offset
                    start_td = timedelta(seconds=start_time)
                    end_td = timedelta(seconds=end_time)
                    start_str = f"{int(start_td.total_seconds()) // 3600:02}:{int(start_td.total_seconds()) % 3600 // 60:02}:{int(start_td.total_seconds()) % 60:02},{start_td.microseconds // 1000:03}"
                    end_str = f"{int(end_td.total_seconds()) // 3600:02}:{int(end_td.total_seconds()) % 3600 // 60:02}:{int(end_td.total_seconds()) % 60:02},{end_td.microseconds // 1000:03}"
                    srt_entry = f"{start_str} --> {end_str}\n{segment['text'].strip()}"
                    srt_entries.append(srt_entry)
                    batch_duration = max(batch_duration, segment['end'])
            
            final_srt_parts.extend(srt_entries)
            total_duration_offset += batch_duration
            logger.info(f"ğŸ“ æ‰¹æ¬¡ {batch_count} è½‰éŒ„å®Œæˆï¼Œ{len(srt_entries)} å€‹ç‰‡æ®µ")

        # 7. ç”Ÿæˆæœ€çµ‚ SRT
        if final_srt_parts:
            srt_path = os.path.join(temp_dir, "final.srt")
            with open(srt_path, "w", encoding="utf-8") as f:
                for i, srt_entry in enumerate(final_srt_parts):
                    f.write(f"{i + 1}\n{srt_entry}\n\n")

            srt_blob_path = f"{base_path}/{SRT_FOLDER}/final.srt"
            srt_blob_path_clean = srt_blob_path.replace(f"{BUCKET_NAME}/", "")
            srt_url = upload_to_gcs(srt_path, srt_blob_path_clean)
            logger.info(f"ğŸ“„ SRT å·²ä¸Šå‚³ï¼š{srt_url}")

            payload = {
                "ä»»å‹™ç‹€æ…‹": "æˆåŠŸ", "user_id": user_id, "task_id": task_id, "video_url": video_url,
                "whisper_language": whisper_language, "srt_url": srt_url, "å½±ç‰‡åŸå§‹å¤§å°MB": total_mb,
                "éŸ³æª”æ‰¹æ¬¡æ•¸": len(audio_chunks), "ç¸½æ™‚é•·ç§’": total_duration_offset,
                "è½‰æ›æ–¹å¼": "Google Transcoder API", "ç¨‹å¼ç‰ˆæœ¬": VERSION,
                "user_email": user_email, "user_name": user_name, "user_headpic": user_headpic,
                "user_lastname": user_lastname,
            }
            requests.post(webhook_url, json=payload, timeout=10)
            logger.info("âœ… ä»»å‹™å®Œæˆ")
        else:
            raise Exception("æ²’æœ‰æˆåŠŸè™•ç†ä»»ä½•éŸ³æª”æ‰¹æ¬¡")

    except Exception as e:
        logger.error(f"ğŸ”¥ ä»»å‹™è™•ç†éŒ¯èª¤ - {e}", exc_info=True)
        payload = {
            "ä»»å‹™ç‹€æ…‹": f"å¤±æ•—: {str(e)}", "user_id": user_id, "task_id": task_id,
            "video_url": video_url, "whisper_language": whisper_language, "srt_url": "",
            "ç¨‹å¼ç‰ˆæœ¬": VERSION, "user_email": user_email, "user_name": user_name,
            "user_headpic": user_headpic, "user_lastname": user_lastname,
        }
        try:
            requests.post(webhook_url, json=payload, timeout=10)
        except Exception as webhook_e:
            logger.error(f"ğŸ”¥ ç™¼é€å¤±æ•—é€šçŸ¥åˆ° Webhook æ™‚ä¹Ÿç™¼ç”ŸéŒ¯èª¤: {webhook_e}")
    finally:
        logger.info(f"ğŸ§¹ æ¸…é™¤æš«å­˜è³‡æ–™å¤¾ï¼š{temp_dir}")
        shutil.rmtree(temp_dir, ignore_errors=True)

# (å…¶é¤˜è¼”åŠ©å‡½å¼ extract_base_path_from_url, convert_http_url_to_gcs_uri ç­‰ä¿æŒä¸è®Š)
# ...
def extract_base_path_from_url(video_url):
    """å¾å½±ç‰‡ URL æå–åŸºç¤è·¯å¾‘"""
    try:
        if video_url.startswith("https://storage.googleapis.com/"):
            gcs_path = video_url.replace("https://storage.googleapis.com/", "")
            base_path = "/".join(gcs_path.split("/")[:-1])
            return base_path
        else:
            raise ValueError(f"URL ä¸æ˜¯æœ‰æ•ˆçš„ GCS HTTP URL: {video_url}")
    except Exception as e:
        logger.error(f"âŒ æå–åŸºç¤è·¯å¾‘å¤±æ•—ï¼š{e}")
        return None

def convert_http_url_to_gcs_uri(http_url):
    """å°‡ HTTP URL è½‰æ›ç‚º GCS URI"""
    try:
        if http_url.startswith("https://storage.googleapis.com/"):
            gcs_path = http_url.replace("https://storage.googleapis.com/", "")
            return f"gs://{gcs_path}"
        else:
            raise ValueError(f"URL ä¸æ˜¯æœ‰æ•ˆçš„ GCS HTTP URL: {http_url}")
    except Exception as e:
        logger.error(f"âŒ URL è½‰æ›å¤±æ•—ï¼š{e}")
        return None

def create_transcoder_job(input_uri, output_folder_uri, job_id):
    """å»ºç«‹ Transcoder ä»»å‹™ä¾†è½‰æ›å½±ç‰‡ç‚º MP3"""
    try:
        logger.info(f"ğŸ¬ å»ºç«‹ Transcoder ä»»å‹™ï¼š{job_id}")
        logger.info(f"ğŸ“¥ è¼¸å…¥ï¼š{input_uri}")
        logger.info(f"ğŸ“¤ è¼¸å‡ºç›®éŒ„ï¼š{output_folder_uri}")
        
        audio_stream = transcoder_v1.AudioStream(
            codec="mp3", bitrate_bps=128000, sample_rate_hertz=44100, channel_count=2
        )
        mux_stream = transcoder_v1.MuxStream(
            key="audio_only", container="mp3", elementary_streams=["audio_stream"]
        )
        job = transcoder_v1.Job(
            input_uri=input_uri, output_uri=output_folder_uri,
            config=transcoder_v1.JobConfig(
                elementary_streams=[
                    transcoder_v1.ElementaryStream(key="audio_stream", audio_stream=audio_stream)
                ],
                mux_streams=[mux_stream]
            )
        )
        parent = f"projects/{PROJECT_ID}/locations/{LOCATION}"
        request = transcoder_v1.CreateJobRequest(parent=parent, job=job)
        created_job = transcoder_client.create_job(request=request)
        logger.info(f"âœ… Transcoder ä»»å‹™å»ºç«‹æˆåŠŸï¼š{created_job.name}")
        return created_job
    except Exception as e:
        logger.error(f"âŒ å»ºç«‹ Transcoder ä»»å‹™å¤±æ•—ï¼š{e}")
        return None

def wait_for_transcoder_job(job_name, timeout_minutes=30):
    """ç­‰å¾… Transcoder ä»»å‹™å®Œæˆ"""
    try:
        logger.info(f"â³ ç­‰å¾… Transcoder ä»»å‹™å®Œæˆï¼š{job_name}")
        timeout_seconds = timeout_minutes * 60
        start_time = time.time()
        while time.time() - start_time < timeout_seconds:
            job = transcoder_client.get_job(name=job_name)
            state_names = {1: "PENDING", 2: "RUNNING", 3: "SUCCEEDED", 4: "FAILED"}
            state_name = state_names.get(job.state, f"UNKNOWN({job.state})")
            logger.info(f"ğŸ“Š ä»»å‹™ç‹€æ…‹ï¼š{state_name}")
            if job.state == 3:
                logger.info("âœ… Transcoder ä»»å‹™å®Œæˆ")
                return True
            elif job.state == 4:
                logger.error(f"âŒ Transcoder ä»»å‹™å¤±æ•—")
                if hasattr(job, 'error') and job.error:
                    logger.error(f"éŒ¯èª¤è©³æƒ…ï¼š{job.error}")
                return False
            time.sleep(30)
        logger.error(f"â° Transcoder ä»»å‹™è¶…æ™‚ ({timeout_minutes} åˆ†é˜)")
        return False
    except Exception as e:
        logger.error(f"âŒ ç­‰å¾… Transcoder ä»»å‹™å¤±æ•—ï¼š{e}")
        return False

def download_audio_from_gcs(gcs_uri, local_path):
    """å¾ GCS ä¸‹è¼‰éŸ³æª”"""
    try:
        if not gcs_uri.startswith("gs://"):
            raise ValueError(f"Invalid GCS URI: {gcs_uri}")
        uri_parts = gcs_uri[5:].split("/", 1)
        bucket = storage_client.bucket(uri_parts[0])
        blob = bucket.blob(uri_parts[1])
        logger.info(f"ğŸ“¥ å¾ GCS ä¸‹è¼‰éŸ³æª”ï¼š{gcs_uri}")
        blob.download_to_filename(local_path)
        size_mb = round(os.path.getsize(local_path) / 1024 / 1024, 2)
        logger.info(f"âœ… éŸ³æª”ä¸‹è¼‰å®Œæˆï¼š{size_mb} MB")
        return True
    except Exception as e:
        logger.error(f"âŒ ä¸‹è¼‰éŸ³æª”å¤±æ•—ï¼š{e}")
        return False

def split_audio_file(audio_path, chunk_size_mb=24):
    """åˆ†å‰²éŸ³æª”ç‚ºå¤šå€‹å°æª”æ¡ˆ"""
    try:
        logger.info(f"ğŸ”ª åˆ†å‰²éŸ³æª”ï¼š{audio_path}")
        cmd = ["ffprobe", "-v", "quiet", "-show_entries", "format=duration", "-of", "csv=p=0", audio_path]
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode != 0:
            raise RuntimeError(f"ç„¡æ³•å–å¾—éŸ³æª”æ™‚é•·ï¼š{result.stderr}")
        total_duration = float(result.stdout.strip())
        file_size_mb = os.path.getsize(audio_path) / 1024 / 1024
        logger.info(f"ğŸ“Š éŸ³æª”ç¸½æ™‚é•·ï¼š{total_duration:.2f}sï¼Œå¤§å°ï¼š{file_size_mb:.2f}MB")
        if file_size_mb <= chunk_size_mb:
            logger.info("ğŸ“¦ éŸ³æª”å¤§å°ç¬¦åˆé™åˆ¶ï¼Œä¸éœ€åˆ†å‰²")
            return [audio_path]
        chunk_duration = (total_duration * chunk_size_mb) / file_size_mb
        num_chunks = int(total_duration / chunk_duration) + 1
        logger.info(f"ğŸ”ª å°‡åˆ†å‰²ç‚º {num_chunks} æ®µï¼Œæ¯æ®µç´„ {chunk_duration:.2f}s")
        chunks = []
        base_path = os.path.splitext(audio_path)[0]
        for i in range(num_chunks):
            start_time = i * chunk_duration
            end_time = min((i + 1) * chunk_duration, total_duration)
            if start_time >= total_duration:
                break
            chunk_path = f"{base_path}_chunk_{i:03d}.mp3"
            cmd = ["ffmpeg", "-y", "-i", audio_path, "-ss", str(start_time), "-t", str(end_time - start_time), "-c", "copy", chunk_path]
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode != 0:
                logger.error(f"âŒ åˆ†å‰²å¤±æ•—ï¼š{result.stderr}")
                continue
            chunks.append(chunk_path)
            chunk_size = round(os.path.getsize(chunk_path) / 1024 / 1024, 2)
            logger.info(f"âœ… åˆ†å‰²å®Œæˆï¼š{os.path.basename(chunk_path)} ({chunk_size} MB)")
        return chunks
    except Exception as e:
        logger.error(f"âŒ åˆ†å‰²éŸ³æª”å¤±æ•—ï¼š{e}")
        return []

def upload_to_gcs(file_path, blob_path):
    """ä¸Šå‚³æª”æ¡ˆåˆ° GCS"""
    try:
        bucket = storage_client.bucket(BUCKET_NAME)
        blob = bucket.blob(blob_path)
        content_type = "application/x-subrip" if file_path.endswith(".srt") else "audio/mpeg"
        blob.upload_from_filename(file_path, content_type=content_type)
        return f"https://storage.googleapis.com/{BUCKET_NAME}/{blob_path}"
    except Exception as e:
        logger.error(f"GCS ä¸Šå‚³å¤±æ•—ï¼š{e}")
        raise
